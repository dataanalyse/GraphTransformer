# Methodology

This document explains the research methodology used in this project: network design, simulation, data preparation, and model training/evaluation.

## 1. Problem Framing

The task is **next-step node health prediction** in a supply network.

- Each node is an entity in the chain/network.
- Each directed edge indicates dependency flow (upstream -> downstream).
- At each time step, each node is either:
  - `1` = healthy (up)
  - `0` = disrupted (down)

Given system state at time `t`, models predict node health at time `t+1`.

## 2. Network Design

The project studies multiple graph structures to test how topology affects model behavior:

- `chain` (simple baseline structure)
- `scale_free` (preferential-attachment style)
- `tiered_scale_free` (domain-constrained, role-aware)

### Tiered Scale-Free Rationale

Pure scale-free graphs can create unrealistic shortcuts (for example direct supplier -> retailer links).  
To improve realism, `tiered_scale_free` enforces directional tier flow:

- supplier tier -> intermediate tier(s) -> retailer tier
- no direct supplier -> retailer edge
- preferential attachment still used within allowed tier transitions

This keeps hub behavior while preserving supply-chain semantics.

## 3. Simulation Dynamics

The data is generated by a **stochastic dynamical process**, not i.i.d. random labels.

At each time step, node states transition from `t` to `t+1` via three rules:

1. `Shock` (exogenous)
- A healthy node may fail with probability `p_shock`.
- Interpretation: external hazards (earthquake, strike, cyber event, policy shock).

2. `Propagation` (endogenous)
- A healthy node may fail with probability `p_propagate * exposure`.
- `exposure` = fraction of disrupted upstream neighbors.
- Interpretation: internal contagion through supply dependencies.

3. `Recovery`
- A disrupted node may recover with probability `p_recover`.

So yes:
- **Shock = exogenous**
- **Propagation = endogenous**

This is the core modeling logic: external hazard + internal contagion + probabilistic recovery.

## 4. Data Preparation

For each node at each time step, the simulator records:

- `health(t)`
- `exposure(t)`
- `time_to_recovery(t)` (down-age proxy)

These are transformed into supervised tensors:

- `X[t, node, feature] = [health, exposure, time_to_recovery] at time t`
- `Y[t, node] = health at time t+1`

If simulation length is `T=200`, tensor time dimension is `T-1=199` because targets are one-step-ahead.

Example shapes:
- N=3: `X=[199,3,3]`, `Y=[199,3]`
- N=5: `X=[199,5,3]`, `Y=[199,5]`
- N=7: `X=[199,7,3]`, `Y=[199,7]`

### Why prediction is possible on simulated data

Although transitions are stochastic, they are **structured** by known conditional rules:

- current node state
- upstream disruption exposure
- graph topology
- fixed parameters (`p_shock`, `p_propagate`, `p_recover`)

Models learn this conditional mapping `P(Y_{t+1} | state_t, graph, params)`, not pure noise.

## 5. Training and Evaluation

Models compared:

- Baseline (logistic regression on node features)
- GCN (message passing over graph edges)
- Graph Transformer (graph-masked self-attention)

Common training setup:

- binary target: healthy vs disrupted at `t+1`
- loss: `BCEWithLogitsLoss`
- class-imbalance adjustment via `pos_weight`
- chronological split:
  - first ~70% timesteps for train
  - last ~30% timesteps for test
- deterministic seed control for reproducibility

Primary metric:
- test accuracy (`last_test_acc`, also tracked over epochs as `best_test_acc`)

## 6. Interpretation and Research Validity

Important methodological point:

- Current experiments validate whether models can learn the simulated process.
- They do **not** by themselves prove real-world predictive validity.

To strengthen scientific claims, use:

1. Multi-seed evaluation (mean/std, not single-seed conclusions)
2. Parameter-shift tests (train/test under different simulator regimes)
3. Topology-shift tests (train on one family, test on another)
4. Sanity checks (label shuffle should collapse performance)

## 7. What This Methodology Delivers

This project now supports a consistent experimental methodology:

- explicit network design choices
- transparent stochastic simulation assumptions
- clear supervised data construction
- comparable training across baseline, GCN, and transformer
- reproducible, logged, topology-aware evaluation

